{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Read the data\n",
    "pareto_models = pd.read_csv(\"pareto_hyperparameters.csv\")\n",
    "print(pareto_models.info())\n",
    "\n",
    "# Assuming 'dt_simulated_weekly' and 'temp' data are also available in CSV files\n",
    "dt_simulated_weekly = pd.read_csv(\"dt_simulated_weekly.csv\")\n",
    "temp = pd.read_csv(\"pareto_alldecomp_matrix.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the models based on a specific criterion\n",
    "sorted_models = pareto_models.sort_values(by=['decomp.rssd'])\n",
    "\n",
    "# Select the best and worst models\n",
    "best_model = sorted_models.iloc[0]\n",
    "worst_model = sorted_models.iloc[-1]\n",
    "\n",
    "print(f\"Best model solID: {best_model['solID']}\")\n",
    "print(f\"Worst model solID: {worst_model['solID']}\")\n",
    "\n",
    "# Get the fits for these models\n",
    "best_model_fits = temp[temp['solID'] == best_model['solID']]\n",
    "worst_model_fits = temp[temp['solID'] == worst_model['solID']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KL Divergence Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate KL Divergence (adjust as needed)\n",
    "def kl_divergence(p, q):\n",
    "    return entropy(p, q)\n",
    "\n",
    "# True values\n",
    "true_vals = dt_simulated_weekly['revenue'][6:163]  # Adjust indices as necessary\n",
    "\n",
    "# Calculate KL Divergence\n",
    "kl_div_best = kl_divergence(true_vals, best_model_fits['depVarHat'])\n",
    "kl_div_worst = kl_divergence(true_vals, worst_model_fits['depVarHat'])\n",
    "\n",
    "print(f\"KL Divergence for Best Model: {kl_div_best}\")\n",
    "print(f\"KL Divergence for Worst Model: {kl_div_worst}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you've calculated KL Divergence for a range of k values as needed\n",
    "# Prepare the dataset for plotting\n",
    "\n",
    "# Plotting code would depend on how the data for multiple k values is structured\n",
    "# Here's a basic example of plotting with seaborn\n",
    "\n",
    "sns.lineplot(data=your_kl_divergence_data, x=\"k\", y=\"KL_divergence\", hue=\"Model\")\n",
    "plt.title(\"KL Divergence of Best and Worst Model Fits\")\n",
    "plt.xlabel(\"Number of neighbours considered\")\n",
    "plt.ylabel(\"KL Divergence\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chebyshev's Inequality Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the empirical probability\n",
    "def empirical_prob(residuals, k):\n",
    "    sigma = np.std(residuals)\n",
    "    count = np.sum(np.abs(residuals) >= k * sigma)\n",
    "    return count / len(residuals)\n",
    "\n",
    "# Assuming residuals are calculated as the difference between predictions and true values\n",
    "residuals_best = best_model_fits['depVarHat'] - true_vals\n",
    "residuals_worst = worst_model_fits['depVarHat'] - true_vals\n",
    "\n",
    "# Calculate empirical probabilities for different values of k\n",
    "ks = [0.25, 0.5, 0.75, 1, np.sqrt(2), 1.5, 1.75, 2, 3, 4, 5]\n",
    "probabilities_best = [empirical_prob(residuals_best, k) for k in ks]\n",
    "probabilities_worst = [empirical_prob(residuals_worst, k) for k in ks]\n",
    "\n",
    "# Theoretical bounds from Chebyshev's inequality\n",
    "chebyshev_bounds = [min(1, 1 / k**2) for k in ks]\n",
    "\n",
    "# Combine and compare the results\n",
    "results_df = pd.DataFrame({\n",
    "    'k': ks,\n",
    "    'Best Model': probabilities_best,\n",
    "    'Worst Model' : probabilities_worst})\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
